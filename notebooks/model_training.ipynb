{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (47965, 784)\n",
      "Validation set size: (11992, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the cleaned dataset\n",
    "train_data_cleaned = pd.read_csv(\"C:/project/fashion-recommender-system/data/processed/fashion-mnist_train_cleaned.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = train_data_cleaned.iloc[:, 1:].values  # Pixel values\n",
    "y = train_data_cleaned.iloc[:, 0].values  # Labels (assuming the first column is the label)\n",
    "\n",
    "# Normalize pixel values (0-255 to 0-1)\n",
    "X = X / 255.0\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Optionally standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Validation set size: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The dataset has been split into training and validation sets, with normalized pixel values between 0 and 1.\n",
    "2. The StandardScaler ensures that pixel intensity values are standardized to improve model convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection and Training (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\om\\anaconda3\\envs\\fashion\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation Accuracy: 0.8568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\om\\anaconda3\\envs\\fashion\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=100, solver='saga', multi_class='multinomial')\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_log_reg = log_reg.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "log_reg_accuracy = accuracy_score(y_val, y_pred_log_reg)\n",
    "print(f\"Logistic Regression Validation Accuracy: {log_reg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic Regression, a basic algorithm, achieved a certain accuracy on the validation set.\n",
    "2. This will serve as a baseline to compare more complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Support Vector Machine - SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Validation Accuracy: 0.8439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize Support Vector Machine model\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_svm = svm_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "svm_accuracy = accuracy_score(y_val, y_pred_svm)\n",
    "print(f\"SVM Validation Accuracy: {svm_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The SVM model with a linear kernel provides a higher accuracy than Logistic Regression, showing better separation of classes.\n",
    "2. This step adds complexity and helps test a more computationally expensive algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Random Forest Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation Accuracy: 0.8797\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_accuracy = accuracy_score(y_val, y_pred_rf)\n",
    "print(f\"Random Forest Validation Accuracy: {rf_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Random Forest classifier provides an ensemble learning approach, which generally improves performance.\n",
    "2. The accuracy can now be compared with other models like Logistic Regression and SVM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Convolutional Neural Network - CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tensorflow as tf\\nfrom tensorflow import keras\\n\\nfrom tensorflow.keras import Sequential\\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\\n# Reshape the data for CNN input (28x28 pixels, 1 channel)\\n# Assuming you have X_train and X_val defined\\nX_train_cnn = X_train.reshape(-1, 28, 28, 1)\\nX_val_cnn = X_val.reshape(-1, 28, 28, 1)\\n\\n# Build a CNN model\\ncnn_model = Sequential([\\n    Conv2D(32, kernel_size=(3, 3), activation=\\'relu\\', input_shape=(28, 28, 1)),  # Added activation and input_shape\\n    MaxPooling2D(pool_size=(2, 2)),  # Corrected to MaxPooling2D\\n    Flatten(),\\n    Dense(10, activation=\\'softmax\\')  # Assuming 10 output classes\\n])\\n\\n# Compile the CNN model\\ncnn_model.compile(optimizer=\\'adam\\', loss=\\'sparse_categorical_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# Train the model\\ncnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=128, validation_data=(X_val_cnn, y_val))\\n\\n# Evaluate the CNN model\\ncnn_eval = cnn_model.evaluate(X_val_cnn, y_val)\\nprint(f\"CNN Validation Accuracy: {cnn_eval[1]:.4f}\")'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "# Reshape the data for CNN input (28x28 pixels, 1 channel)\n",
    "# Assuming you have X_train and X_val defined\n",
    "X_train_cnn = X_train.reshape(-1, 28, 28, 1)\n",
    "X_val_cnn = X_val.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Build a CNN model\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),  # Added activation and input_shape\n",
    "    MaxPooling2D(pool_size=(2, 2)),  # Corrected to MaxPooling2D\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')  # Assuming 10 output classes\n",
    "])\n",
    "\n",
    "# Compile the CNN model\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=128, validation_data=(X_val_cnn, y_val))\n",
    "\n",
    "# Evaluate the CNN model\n",
    "cnn_eval = cnn_model.evaluate(X_val_cnn, y_val)\n",
    "print(f\"CNN Validation Accuracy: {cnn_eval[1]:.4f}\")\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The CNN model uses deep learning to improve the classification accuracy, particularly for image-based datasets.\n",
    "2. This model introduces layers of abstraction for feature extraction and performs better than traditional machine learning models in most cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning (Using Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best Parameters: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Random Forest Validation Accuracy: 0.8793\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize Grid Search\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the model using grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "best_rf_accuracy = grid_search.score(X_val, y_val)\n",
    "print(f\"Best Random Forest Validation Accuracy: {best_rf_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hyperparameter tuning via Grid Search allows us to optimize the Random Forest model for better performance.\n",
    "2. This step systematically tests combinations of parameters for the highest validation accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation (Confusion Matrix and Classification Report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Confusion matrix and classification report for the best performing model (assume CNN)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m rf_model_predictions \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_val)\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generate confusion matrix\u001b[39;00m\n\u001b[0;32m      7\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(y_val, rf_model_predictions)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Confusion matrix and classification report for the best performing model (assume CNN)\n",
    "rf_model_predictions = rf_model.predict(X_val).argmax(axis=1)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, rf_model_predictions)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y_val, rf_model_predictions)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The confusion matrix and classification report show how well the model performs in classifying each label, providing insight into misclassifications.\n",
    "2. This is crucial for understanding model performance beyond accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_rf_model.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best performing model (assume CNN)\n",
    "#rf_model.save('best_rf_model.h5')\n",
    "\n",
    "# Save a traditional machine learning model like Random Forest\n",
    "joblib.dump(grid_search.best_estimator_, 'best_rf_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
